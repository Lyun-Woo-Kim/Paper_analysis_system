# llm_engine.py
import torch
import gc
import sys
import os
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

# í˜„ì¬ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
root_dir = current_dir.parent
sys.path.insert(0, str(root_dir))

from utils.utils import load_config

class QwenLLMEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ model_config.yaml ê²½ë¡œ ì„¤ì •
        config_path = root_dir / "model_config.yaml"
        self.model_id = load_config(str(config_path))["LLM_MODEL_NAME"]

    def is_loaded(self):
        """ëª¨ë¸ì´ í˜„ì¬ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ì™€ ìˆëŠ”ì§€ í™•ì¸"""
        return self.model is not None

    def load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ GPUì— ë¡œë“œ"""
        if self.is_loaded():
            print("âœ… Model is already loaded.")
            return

        print(f"ğŸš€ Loading model: {self.model_id}...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype="auto",
                device_map="auto",
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_id, 
                trust_remote_code=True
            )
            print("âœ… Model loaded successfully!")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise e

    def unload_model(self):
        """ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•˜ê³  GPU ìºì‹œ ì •ë¦¬ (í•µì‹¬ ê¸°ëŠ¥)"""
        if not self.is_loaded():
            print("âš ï¸ Model is not loaded.")
            return

        print("â™»ï¸ Unloading model and clearing GPU memory...")
        
        # ê°ì²´ ì‚­ì œ
        del self.model
        del self.tokenizer
        
        # ì°¸ì¡° ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° CUDA ìºì‹œ ë¹„ìš°ê¸°
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        
        print("âœ… GPU memory cleared!")

    def generate_response(self, messages):
        """í…ìŠ¤íŠ¸ ì¶”ë¡  ìˆ˜í–‰"""
        if not self.is_loaded():
            # í¸ì˜ë¥¼ ìœ„í•´ ë¡œë“œê°€ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œ
            print("ğŸ”„ Model not loaded. Auto-loading...")
            self.load_model()

        # 1. ì…ë ¥ ì „ì²˜ë¦¬ (chat template ì ìš©)
        text = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # 2. í…ì„œ ë³€í™˜
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
        ).to("cuda")

        # 3. ì¶”ë¡  (Inference)
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=1024
            )

        # 4. ê²°ê³¼ ë””ì½”ë”©
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.tokenizer.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        return output_text

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
engine = QwenLLMEngine()

