{"label": "Table", "caption": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.", "summary": "This table presents experimental results for different configurations of the Transformer model, showing how varying hyperparameters like model size, dropout rates, and positional encoding affect PPL and BLEU scores on a translation task.", "key_points": ["The 'base' model achieves a PPL of 4.92 and BLEU of 25.8.", "Increasing model size (e.g., d_model from 512 to 1024) generally improves BLEU scores, with the 'big' model reaching 26.4.", "Using positional embedding instead of sinusoids slightly improves BLEU to 25.7.", "The 'big' model with 1024 dimensions, 4096 hidden size, 16 heads, and 0.3 dropout achieves the best performance with PPL 4.33 and BLEU 26.4.", "Smaller models with fewer parameters (e.g., 36k) often have lower BLEU scores than larger models (e.g., 168k)."], "evidence": ["base", "N 6, d_model 512, d_ff 2048, h 8, d_k 64, d_v 64, P_drop 0.1, e_ls 0.1, train steps 100K, PPL 4.92, BLEU 25.8, params 65k", "big", "N 6, d_model 1024, d_ff 4096, h 16, d_k 64, d_v 64, P_drop 0.3, e_ls 0.1, train steps 300K, PPL 4.33, BLEU 26.4, params 213k", "PPL (dev)", "BLEU (dev)", "params ×10^6", "position embedding instead of sinusoids"], "headers": {"columns": ["", "N", "d_model", "d_ff", "h", "d_k", "d_v", "P_drop", "e_ls", "train steps", "PPL (dev)", "BLEU (dev)", "params ×10^6"], "rows": ["base", "(A)", "(B)", "(C)", "(D)", "(E)", "big"]}, "best": {"what": "BLEU score", "where": "big model", "value": "26.4"}, "numbers": [{"name": "PPL (dev)", "value": "4.33", "unit": "unknown", "context": "big model"}, {"name": "BLEU (dev)", "value": "26.4", "unit": "unknown", "context": "big model"}, {"name": "params ×10^6", "value": "213", "unit": "unknown", "context": "big model"}, {"name": "train steps", "value": "300K", "unit": "unknown", "context": "big model"}, {"name": "d_model", "value": "1024", "unit": "unknown", "context": "big model"}], "page_index": 8}
{"label": "Table", "caption": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.", "summary": "The Transformer model outperforms previous models in BLEU scores for both EN-DE and EN-FR translation while requiring significantly fewer FLOPs for training.", "key_points": ["Transformer (big) achieves the highest BLEU scores (28.4 for EN-DE, 41.8 for EN-FR).", "Transformer (big) requires 2.3 × 10^19 FLOPs, much lower than previous models like ConvS2S Ensemble (7.7 × 10^19 FLOPs).", "Transformer (base model) also shows strong performance with 27.3 BLEU for EN-DE and 38.1 for EN-FR."], "evidence": ["BLEU EN-DE for Transformer (big): 28.4", "BLEU EN-FR for Transformer (big): 41.8", "Training Cost EN-DE for Transformer (big): 2.3 · 10^19", "Training Cost EN-FR for Transformer (big): 2.3 · 10^19", "ConvS2S Ensemble [9] BLEU EN-FR: 41.29", "ConvS2S Ensemble [9] Training Cost EN-FR: 1.2 · 10^21", "ByteNet [18] BLEU EN-DE: 23.75", "Deep-Att + PosUnk Ensemble [39] BLEU EN-FR: 40.4"], "headers": {"columns": ["Model", "BLEU EN-DE", "BLEU EN-FR", "Training Cost (FLOPs) EN-DE", "Training Cost (FLOPs) EN-FR"], "rows": ["ByteNet [18]", "Deep-Att + PosUnk [39]", "GNMT + RL [38]", "ConvS2S Ensemble [9]", "MoE [32]", "Deep-Att + PosUnk Ensemble [39]", "GNMT + RL Ensemble [38]", "ConvS2S Ensemble [9]", "Transformer (base model)", "Transformer (big)"]}, "best": {"what": "BLEU score", "where": "Transformer (big) for EN-DE and EN-FR", "value": "28.4 and 41.8"}, "numbers": [{"name": "Transformer (big) BLEU EN-DE", "value": "28.4", "unit": "unknown", "context": "highest BLEU score for EN-DE"}, {"name": "Transformer (big) BLEU EN-FR", "value": "41.8", "unit": "unknown", "context": "highest BLEU score for EN-FR"}, {"name": "Transformer (big) Training Cost EN-DE", "value": "2.3 · 10^19", "unit": "FLOPs", "context": "lowest training cost among top models"}, {"name": "Transformer (big) Training Cost EN-FR", "value": "2.3 · 10^19", "unit": "FLOPs", "context": "lowest training cost among top models"}, {"name": "ConvS2S Ensemble [9] Training Cost EN-FR", "value": "1.2 · 10^21", "unit": "FLOPs", "context": "much higher training cost than Transformer"}], "page_index": 7}
{"label": "Figure", "caption": "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.", "summary": "The figure displays two attention head visualizations for a sentence, showing how different heads connect words in distinct patterns, indicating they perform different tasks.", "key_points": ["Two attention head visualizations are shown, one in green and one in red.", "Each visualization connects source words (left) to target words (right) with lines indicating attention.", "The green visualization shows more complex connections, while the red one shows simpler, more direct connections.", "Words like 'The', 'Law', 'will', 'never', 'be', 'perfect', 'application', 'should', 'just', 'this', 'is', 'what', 'we', 'are', 'missing', 'in', 'my', 'opinion', '<EOS>', '<pad>' are visible as labels.", "The figure suggests different attention heads learn to perform different tasks."], "evidence": ["The", "Law", "will", "never", "be", "perfect", "application", "should", "just", "this", "is", "what", "we", "are", "missing", "in", "my", "opinion", "<EOS>", "<pad>"], "numbers": [], "page_index": 14}
{"label": "Figure", "caption": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.", "summary": "This diagram illustrates how attention mechanisms in a transformer model focus on distant words to complete phrases, specifically showing how the word 'making' connects to 'more difficult' across the sentence. Different colored lines represent different attention heads.", "key_points": ["Attention heads connect 'making' to distant words like 'more' and 'difficult'.", "The diagram highlights long-distance dependencies in sentence processing.", "Different colors represent different attention heads."], "evidence": ["It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult", "making", "more", "difficult", "the", "registration", "or", "voting", "process", "<EOS>", "<pad>"], "numbers": [{"name": "layer", "value": "5", "unit": "unknown", "context": "layer 5 of 6"}, {"name": "year", "value": "2009", "unit": "unknown", "context": "since 2009"}], "page_index": 12}
{"label": "Figure", "caption": "Scaled Dot-Product Attention Multi-Head Attention", "summary": "The diagram illustrates the computational flow of scaled dot-product attention, showing how query (Q), key (K), and value (V) tensors are processed through matmul, scaling, masking, and softmax operations to produce an output.", "key_points": ["The process begins with two MatMul operations, one for Q*K and another for the final output.", "The output of the first MatMul is scaled and optionally masked before passing to SoftMax.", "The SoftMax result is then multiplied by V to produce the final output.", "The diagram shows Q, K, and V as inputs to the respective MatMul operations.", "The operations are arranged in a vertical flow with arrows indicating data direction."], "evidence": ["MatMul", "SoftMax", "Mask (opt.)", "Scale", "Q", "K", "V", "arrows indicating flow"], "numbers": [], "page_index": 3}
{"label": "Table", "caption": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.", "summary": "The table compares computational complexity, sequential operations, and maximum path length for four different layer types: Self-Attention, Recurrent, Convolutional, and Self-Attention (restricted).", "key_points": ["Self-Attention has O(n²·d) complexity and O(1) sequential operations.", "Recurrent layers have O(n·d²) complexity and O(n) sequential operations.", "Convolutional layers have O(k·n·d²) complexity and O(1) sequential operations, with a maximum path length of O(logₖ(n)).", "Restricted Self-Attention has O(r·n·d) complexity and O(1) sequential operations, with a maximum path length of O(n/r)."], "evidence": ["Layer Type", "Complexity per Layer", "Sequential Operations", "Maximum Path Length", "Self-Attention", "O(n²·d)", "O(1)", "O(1)", "Recurrent", "O(n·d²)", "O(n)", "O(n)", "Convolutional", "O(k·n·d²)", "O(1)", "O(logₖ(n))", "Self-Attention (restricted)", "O(r·n·d)", "O(1)", "O(n/r)"], "headers": {"columns": ["Layer Type", "Complexity per Layer", "Sequential Operations", "Maximum Path Length"], "rows": ["Self-Attention", "Recurrent", "Convolutional", "Self-Attention (restricted)"]}, "best": {"what": "unknown", "where": "unknown", "value": "unknown"}, "numbers": [{"name": "n", "value": "sequence length", "unit": "unknown", "context": "used in complexity and path length formulas"}, {"name": "d", "value": "representation dimension", "unit": "unknown", "context": "used in complexity and path length formulas"}, {"name": "k", "value": "kernel size of convolutions", "unit": "unknown", "context": "used in complexity and path length formulas"}, {"name": "r", "value": "size of the neighborhood in restricted self-attention", "unit": "unknown", "context": "used in complexity and path length formulas"}], "page_index": 5}
{"label": "Figure", "caption": "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.", "summary": "The figure displays attention weights for two transformer attention heads (likely head 5 and head 6) in layer 5 of a 6-layer model, focusing on anaphora resolution. The top panel shows full attention patterns, while the bottom panel isolates attention from the word 'its', revealing sharp, focused connections.", "key_points": ["Attention is shown for two transformer heads (head 5 and head 6) in layer 5 of a 6-layer model.", "The bottom panel isolates attention weights originating from the word 'its'.", "The attention patterns are described as 'very sharp' for the word 'its'."], "evidence": ["The", "Law", "will", "never", "be", "perfect", "but", "its", "application", "should", "be", "just", "this", "is", "what", "we", "are", "missing", "in", "my", "opinion", "<EOS>", "<pad>", "head 5", "head 6", "layer 5 of 6", "anaphora resolution", "very sharp"], "numbers": [{"name": "layer", "value": "5", "unit": "unknown", "context": "Layer 5 of a 6-layer transformer model"}, {"name": "total layers", "value": "6", "unit": "unknown", "context": "Total number of layers in the transformer model"}], "page_index": 13}
{"label": "Table", "caption": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)", "summary": "This table compares the performance of various parsers on the WSJ 23 F1 metric, showing results under different training conditions such as discriminative, semi-supervised, and multi-task generative training.", "key_points": ["The Transformer (4 layers) achieves the highest F1 score of 93.2 with semi-supervised training.", "Dyer et al. (2016) [8] achieves the best score of 93.3 with multi-task generative training.", "All parsers listed achieve scores above 90.0, indicating strong performance."], "evidence": ["Parser: Vinyals & Kaiser et al. (2014) [37]", "Training: WSJ only, discriminative", "WSJ 23 F1: 88.3", "Parser: Dyer et al. (2016) [8]", "Training: WSJ only, discriminative", "WSJ 23 F1: 91.7", "Parser: Transformer (4 layers)", "Training: semi-supervised", "WSJ 23 F1: 92.7"], "headers": {"columns": ["Parser", "Training", "WSJ 23 F1"], "rows": ["Vinyals & Kaiser et al. (2014) [37]", "Petrov et al. (2006) [29]", "Zhu et al. (2013) [40]", "Dyer et al. (2016) [8]", "Transformer (4 layers)", "Zhu et al. (2013) [40]", "Huang & Harper (2009) [14]", "McCluskey et al. (2006) [26]", "Vinyals & Kaiser et al. (2014) [37]", "Transformer (4 layers)", "Luong et al. (2015) [23]", "Dyer et al. (2016) [8]"]}, "best": {"what": "WSJ 23 F1 score", "where": "Dyer et al. (2016) [8] with multi-task generative training", "value": "93.3"}, "numbers": [{"name": "WSJ 23 F1 score for Dyer et al. (2016) [8] with multi-task generative training", "value": "93.3", "unit": "unknown", "context": "Best overall performance"}, {"name": "WSJ 23 F1 score for Transformer (4 layers) with semi-supervised training", "value": "92.7", "unit": "unknown", "context": "Second best performance"}, {"name": "WSJ 23 F1 score for Dyer et al. (2016) [8] with WSJ only, discriminative training", "value": "91.7", "unit": "unknown", "context": "Performance with discriminative training"}, {"name": "WSJ 23 F1 score for Zhu et al. (2013) [40] with semi-supervised training", "value": "92.1", "unit": "unknown", "context": "Performance with semi-supervised training"}, {"name": "WSJ 23 F1 score for Petrov et al. (2006) [29]", "value": "90.4", "unit": "unknown", "context": "Performance with discriminative training"}], "page_index": 9}
{"label": "Figure", "caption": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.", "summary": "The diagram illustrates the architecture of Multi-Head Attention, showing multiple parallel Scaled Dot-Product Attention layers that process inputs V, K, and Q, then concatenate and apply a linear transformation.", "key_points": ["Multi-Head Attention is composed of multiple parallel Scaled Dot-Product Attention heads.", "Each head processes inputs V, K, and Q through linear transformations.", "The outputs of the heads are concatenated and then passed through a final linear layer."], "evidence": ["Multi-Head Attention", "Scaled Dot-Product Attention", "Concat", "Linear", "V", "K", "Q", "h"], "numbers": [], "page_index": 3}
{"label": "Figure", "caption": "Figure 1: The Transformer - model architecture.", "summary": "The diagram illustrates the architecture of the Transformer model, showing its encoder and decoder components, each containing multiple identical layers. The encoder processes input embeddings with positional encoding, while the decoder processes output embeddings with positional encoding and uses masked attention to handle sequence generation. Both encoder and decoder layers include multi-head attention and feed-forward networks, with residual connections and layer normalization.", "key_points": ["The model consists of an encoder and a decoder, each with N identical layers.", "Each layer contains multi-head attention and a feed-forward network, with residual connections and layer normalization.", "The decoder uses masked multi-head attention to prevent attending to future positions."], "evidence": ["Output Probabilities", "Softmax", "Linear", "Add & Norm", "Feed Forward", "Multi-Head Attention", "Masked Multi-Head Attention", "Positional Encoding", "Input Embedding", "Output Embedding", "Inputs", "Outputs (shifted right)", "N×"], "numbers": [{"name": "Number of encoder layers", "value": "N", "unit": "unknown", "context": "Shown as 'N×' next to the encoder block"}, {"name": "Number of decoder layers", "value": "N", "unit": "unknown", "context": "Shown as 'N×' next to the decoder block"}], "page_index": 2}
